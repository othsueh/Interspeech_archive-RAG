{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_and_cache_webpage(url, cache_dir=\"cached_pages\"):\n",
    "    # Create cache directory if it doesn't exist\n",
    "    Path(cache_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a filename from the URL (you might want to hash it for longer URLs)\n",
    "    cache_file = Path(cache_dir) / f\"{url.split('/')[-2]}.html\"\n",
    "    \n",
    "    # If cached version exists and is not too old, load it\n",
    "    if cache_file.exists():\n",
    "        print(\"Loading from cache...\")\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    else:\n",
    "        print(\"Fetching from web...\")\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        # Save to cache\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "    \n",
    "    return html_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache...\n"
     ]
    }
   ],
   "source": [
    "# Use the cached version\n",
    "url = \"https://www.isca-archive.org/interspeech_2023/\"\n",
    "html_content = fetch_and_cache_webpage(url)\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "\n",
    "conference_structure = {\n",
    "        'sessions': {}\n",
    "    }\n",
    "    \n",
    "# Find all h4 headers (session titles)\n",
    "sessions = soup.find_all('h4', class_='w3-center')\n",
    "for session in sessions:\n",
    "    session_title = session.text.strip()\n",
    "    papers = []\n",
    "    current = session.find_next_sibling()\n",
    "    \n",
    "    while current and current.name != 'h4':\n",
    "        if current.name == 'a' and 'w3-text' in current.get('class', []):\n",
    "            # Get the paper link\n",
    "            #paper_link = current.get('href', '')\n",
    "            paper_content = current.find_next('p')\n",
    "            br_tag = paper_content.find('br')\n",
    "            papers.append(''.join(br_tag.previous_siblings).strip())\n",
    "            # Find associated paper details by following the link\n",
    "            #paper_details = get_paper_details(paper_link)  # You would implement this\n",
    "            #papers.append(paper_details)\n",
    "        \n",
    "        current = current.find_next_sibling()\n",
    "    \n",
    "    conference_structure['sessions'][session_title] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to formatted text\n",
    "def dict_to_text(session_dict):\n",
    "    if not session_dict:\n",
    "        return \"\"\n",
    "    text_parts = []\n",
    "    for session_name, papers in session_dict.items():\n",
    "        text_parts.append(f\"Session: {session_name}\")\n",
    "        if papers:  # Check if papers list is not empty\n",
    "            for paper in papers:\n",
    "                if isinstance(paper, str):\n",
    "                    text_parts.append(f\"Paper: {paper}\")\n",
    "                elif isinstance(paper, dict):  # If paper is a dictionary\n",
    "                    text_parts.append(f\"Paper: {paper.get('title', '')}\")\n",
    "                    text_parts.append(f\"Abstract: {paper.get('abstract', '')}\")\n",
    "    return \"\\n\".join(text_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "    )\n",
    "chunks = json_splitter.split_json(conference_structure)\n",
    "texts = [dict_to_text(chunk.get(\"sessions\")) for chunk in chunks]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "vectorstore.save_local(\"Interspeech2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved FAISS index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"Interspeech2023\",  # The path where you saved the index\n",
    "    allow_dangerous_deserialization=True,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "# Create a retriever \n",
    "retriever = VectorStoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 7},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the similarity search\n",
    "results = vectorstore.similarity_search(query, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context. Here are some examples of how to analyze and answer questions:\n",
    "\n",
    "Example 1:\n",
    "Context: Session: Speech Recognition\n",
    "Papers: \n",
    "- \"Improving ASR with Deep Learning\"\n",
    "- \"Novel Approaches to Acoustic Modeling\"\n",
    "Question: What is the main focus of the Speech Recognition session?\n",
    "Thought Process:\n",
    "1. Looking at paper titles in the session\n",
    "2. Identifying common themes\n",
    "3. Synthesizing main focus\n",
    "Answer: The Speech Recognition session focuses on advancing ASR technology through deep learning and acoustic modeling improvements.\n",
    "\n",
    "Example 3:\n",
    "Context: Session: Keynote 1 ISCA Medallist\\nPaper: Bridging Speech Science and Technology â\\x80\\x94 Now and Into the Future\\nSession: Speech Synthesis: Prosody and Emotion\n",
    "Session: Keynote 2\\nPaper: Beyond the AI hype: Balancing Innovation and Social Responsibility\\nSession: Paralinguistics\n",
    "Session: Keynote 3\\nPaper: Whatâ\\x80\\x99s in a Rise? The Relevance of Intonation for Attention Orienting\\nSession: Speech Synthesis: Controllability and Adaptation\n",
    "Question: What are the topics of the keynote session?\n",
    "Thought Process:\n",
    "1. Examining session titles\n",
    "2. Identifying main themes\n",
    "3. Summarizing key topics\n",
    "Answer: The topics of the keynote session include \"Balancing Innovation and Social Responsibility,\" \"The Relevance of Intonation for Attention Orienting,\" and \"Bridging Speech Science and Technology â Now and Into the Future.\"\n",
    "\n",
    "\n",
    "Now, please answer the following:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Thought Process:\n",
    "1) First, let's examine the relevant information from the context\n",
    "2) Then, identify key patterns or themes\n",
    "3) Finally, formulate a comprehensive answer\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "query = \"\"\"\n",
    "I am interested in the recent trends in emotion recognition, here I want you to help me find the trends in the recent years.\n",
    "First: Scan Sessions' titles and find relevant content of \"Emotion Recognition\", \"Affective computing\", \"Emotion Analysis\",\"Emotion Detection\" and \"Sentiment Analysis\". \n",
    "Second: Analyze the papers in the sessions and find the common themes.\n",
    "Third: List the sessions and their corresponding common themes, below is the format.\n",
    "Fourth: Summarize the trends in emotion recognition in this year at the end.\n",
    "<format>\n",
    "Session: session_name\n",
    "common themes: theme1, theme2, theme3\n",
    "Session: session_name\n",
    "common themes: theme1, theme2, theme3\n",
    "...\n",
    "</format>\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the chain\n",
    "# Now you can invoke the chain\n",
    "response = qa_chain.invoke(query)\n",
    "with open(\"interspeech2023.txt\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
