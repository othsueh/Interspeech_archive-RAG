{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_and_cache_webpage(url, cache_dir=\"cached_pages\"):\n",
    "    # Create cache directory if it doesn't exist\n",
    "    Path(cache_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a filename from the URL (you might want to hash it for longer URLs)\n",
    "    cache_file = Path(cache_dir) / f\"{url.split('/')[-2]}.html\"\n",
    "    \n",
    "    # If cached version exists and is not too old, load it\n",
    "    if cache_file.exists():\n",
    "        print(\"Loading from cache...\")\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    else:\n",
    "        print(\"Fetching from web...\")\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "        # Save to cache\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "    \n",
    "    return html_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from web...\n"
     ]
    }
   ],
   "source": [
    "# Use the cached version\n",
    "url = \"https://www.isca-archive.org/interspeech_2023/\"\n",
    "html_content = fetch_and_cache_webpage(url)\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.decompose()\n",
    "\n",
    "conference_structure = {\n",
    "        'sessions': {}\n",
    "    }\n",
    "    \n",
    "# Find all h4 headers (session titles)\n",
    "sessions = soup.find_all('h4', class_='w3-center')\n",
    "for session in sessions:\n",
    "    session_title = session.text.strip()\n",
    "    papers = []\n",
    "    current = session.find_next_sibling()\n",
    "    \n",
    "    while current and current.name != 'h4':\n",
    "        if current.name == 'a' and 'w3-text' in current.get('class', []):\n",
    "            # Get the paper link\n",
    "            #paper_link = current.get('href', '')\n",
    "            paper_content = current.find_next('p')\n",
    "            br_tag = paper_content.find('br')\n",
    "            papers.append(''.join(br_tag.previous_siblings).strip())\n",
    "            # Find associated paper details by following the link\n",
    "            #paper_details = get_paper_details(paper_link)  # You would implement this\n",
    "            #papers.append(paper_details)\n",
    "        \n",
    "        current = current.find_next_sibling()\n",
    "    \n",
    "    conference_structure['sessions'][session_title] = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to formatted text\n",
    "def dict_to_text(session_dict):\n",
    "    if not session_dict:\n",
    "        return \"\"\n",
    "    text_parts = []\n",
    "    for session_name, papers in session_dict.items():\n",
    "        text_parts.append(f\"Session: {session_name}\")\n",
    "        if papers:  # Check if papers list is not empty\n",
    "            for paper in papers:\n",
    "                if isinstance(paper, str):\n",
    "                    text_parts.append(f\"Paper: {paper}\")\n",
    "                elif isinstance(paper, dict):  # If paper is a dictionary\n",
    "                    text_parts.append(f\"Paper: {paper.get('title', '')}\")\n",
    "                    text_parts.append(f\"Abstract: {paper.get('abstract', '')}\")\n",
    "    return \"\\n\".join(text_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13262/1817672854.py:6: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "    )\n",
    "chunks = json_splitter.split_json(conference_structure)\n",
    "texts = [dict_to_text(chunk.get(\"sessions\")) for chunk in chunks]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "vectorstore.save_local(\"Interspeech2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved FAISS index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.load_local(\n",
    "    folder_path=\"Interspeech2023\",  # The path where you saved the index\n",
    "    allow_dangerous_deserialization=True,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "# Create a retriever \n",
    "retriever = VectorStoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 7},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the similarity search\n",
    "results = vectorstore.similarity_search(query, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Session: Speech Emotion Recognition 2\\nPaper: A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation\\nPaper: MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer\\nPaper: The co-use of laughter and head gestures across speech styles\\nPaper: EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition\\nPaper: Pre-Finetuning for Few-Shot Emotional Speech Recognition\\nPaper: Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations\\nPaper: Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection\\nPaper: Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews\\nPaper: Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh\\nPaper: Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition\\nPaper: Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models\\nPaper: Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining\\nPaper: Investigating Acoustic Cues for Multilingual Abuse Detection\\nPaper: A novel frequency warping scale for speech emotion recognition\\nPaper: Multi-Scale Temporal Transformer For Speech Emotion Recognition\\nPaper: Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario\\nPaper: A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation'),\n",
       " Document(page_content='Session: Speech Emotion Recognition 1\\nPaper: Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition\\nPaper: The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers\\nPaper: A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model\\nPaper: Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack\\nPaper: Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition\\nPaper: Stable Speech Emotion Recognition with Head-k-Pooling Loss\\nSession: Show and Tell: Health applications and emotion recognition\\nPaper: A Personalised Speech Communication Application for Dysarthric Speakers\\nPaper: Video Multimodal Emotion Recognition System for Real World Applications\\nPaper: Promoting Mental Self-Disclosure in a Spoken Dialogue System\\nPaper: \"Select language, modality or put on a mask!\" Experiments with Multimodal Emotion Recognition\\nPaper: My Vowels Matter: Formant Automation Tools for Diverse Child Speech\\nPaper: NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments\\nPaper: When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do\\nPaper: Stuttering Detection Application\\nPaper: Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games\\nPaper: Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring\\nPaper: 5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids\\nPaper: Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid'),\n",
       " Document(page_content='Session: Keynote 1 ISCA Medallist\\nPaper: Bridging Speech Science and Technology â\\x80\\x94 Now and Into the Future\\nSession: Speech Synthesis: Prosody and Emotion\\nPaper: Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks\\nPaper: Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations\\nPaper: EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis\\nPaper: Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus\\nPaper: Explicit Intensity Control for Accented Text-to-speech\\nPaper: Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech\\nSession: Statistical Machine Translation\\nPaper: Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer\\nPaper: Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters\\nPaper: StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation\\nPaper: Joint Speech Translation and Named Entity Recognition\\nPaper: Analysis of Acoustic information in End-to-End Spoken Language Translation\\nPaper: LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers'),\n",
       " Document(page_content='Session: Keynote 2\\nPaper: Beyond the AI hype: Balancing Innovation and Social Responsibility\\nSession: Paralinguistics 1\\nPaper: Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach\\nPaper: Detection of Laughter and Screaming Using the Attention and CTC Models\\nPaper: Capturing Formality in Speech Across Domains and Languages\\nPaper: Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio\\nPaper: Cues to next-speaker projection in conversational Swedish: Evidence from reaction times\\nPaper: Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech\\nSession: Speech Enhancement and Denoising\\nPaper: Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation\\nPaper: TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective\\nPaper: MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement\\nPaper: Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement\\nPaper: Multi-input Multi-output Complex Spectral Mapping for Speaker Separation\\nPaper: Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain'),\n",
       " Document(page_content='Session: Speech Signal Analysis\\nPaper: MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion\\nPaper: Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset\\nPaper: Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study\\nPaper: Improved Contextualized Speech Representations for Tonal Analysis\\nPaper: A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence\\nPaper: FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals\\nSession: Speech Emotion Recognition 3\\nPaper: Improving Joint Speech and Emotion Recognition Using Global Style Tokens\\nPaper: Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute\\nPaper: Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition\\nPaper: Dual Memory Fusion for Multimodal Speech Emotion Recognition\\nPaper: Hybrid Dataset for Speech Emotion Recognition in Russian Language\\nPaper: Speech Emotion Recognition using Decomposed Speech via Multi-task Learning'),\n",
       " Document(page_content='Session: Paralinguistics 2\\nPaper: Speaker Embeddings as Individuality Proxy for Voice Stress Detection\\nPaper: From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion\\nPaper: Turbo your multi-modal classification with contrastive learning\\nPaper: Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition\\nPaper: SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition\\nPaper: On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition\\nPaper: Speaking State Decoder with Transition Detection for Next Speaker Prediction\\nPaper: What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation\\nPaper: Effects of perceived gender on the perceived social function of laughter\\nPaper: Implicit phonetic information modeling for speech emotion recognition\\nPaper: Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters\\nPaper: Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions\\nPaper: Preference Learning Labels by Anchoring on Consecutive Annotations\\nPaper: Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks\\nPaper: Learning Local to Global Feature Aggregation for Speech Emotion Recognition\\nPaper: Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition'),\n",
       " Document(page_content='Session: Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1\\nPaper: Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text\\nPaper: ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition\\nPaper: BASS: Block-wise Adaptation for Speech Summarization\\nPaper: Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus\\nPaper: Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion\\nPaper: A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition\\nPaper: Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm\\nPaper: Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction\\nPaper: Query Based Acoustic Summarization for Podcasts\\nPaper: Spot Keywords From Very Noisy and Mixed Speech\\nPaper: Knowledge Distillation on Joint Task End-to-End Speech Translation\\nPaper: Investigating Pre-trained Audio Encoders in the Low-Resource Condition\\nPaper: Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context. Here are some examples of how to analyze and answer questions:\n",
    "\n",
    "Example 1:\n",
    "Context: Session: Speech Recognition\n",
    "Papers: \n",
    "- \"Improving ASR with Deep Learning\"\n",
    "- \"Novel Approaches to Acoustic Modeling\"\n",
    "Question: What is the main focus of the Speech Recognition session?\n",
    "Thought Process:\n",
    "1. Looking at paper titles in the session\n",
    "2. Identifying common themes\n",
    "3. Synthesizing main focus\n",
    "Answer: The Speech Recognition session focuses on advancing ASR technology through deep learning and acoustic modeling improvements.\n",
    "\n",
    "Example 3:\n",
    "Context: Session: Keynote 1 ISCA Medallist\\nPaper: Bridging Speech Science and Technology â\\x80\\x94 Now and Into the Future\\nSession: Speech Synthesis: Prosody and Emotion\n",
    "Session: Keynote 2\\nPaper: Beyond the AI hype: Balancing Innovation and Social Responsibility\\nSession: Paralinguistics\n",
    "Session: Keynote 3\\nPaper: Whatâ\\x80\\x99s in a Rise? The Relevance of Intonation for Attention Orienting\\nSession: Speech Synthesis: Controllability and Adaptation\n",
    "Question: What are the topics of the keynote session?\n",
    "Thought Process:\n",
    "1. Examining session titles\n",
    "2. Identifying main themes\n",
    "3. Summarizing key topics\n",
    "Answer: The topics of the keynote session include \"Balancing Innovation and Social Responsibility,\" \"The Relevance of Intonation for Attention Orienting,\" and \"Bridging Speech Science and Technology â Now and Into the Future.\"\n",
    "\n",
    "\n",
    "Now, please answer the following:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Thought Process:\n",
    "1) First, let's examine the relevant information from the context\n",
    "2) Then, identify key patterns or themes\n",
    "3) Finally, formulate a comprehensive answer\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "query = \"\"\"\n",
    "I am interested in the recent trends in emotion recognition, here I want you to help me find the trends in the recent years.\n",
    "First: Scan Sessions' titles and find relevant content of \"Emotion Recognition\", \"Affective computing\", \"Emotion Analysis\",\"Emotion Detection\" and \"Sentiment Analysis\". \n",
    "Second: Analyze the papers in the sessions and find the common themes.\n",
    "Third: List the sessions and their corresponding common themes, below is the format.\n",
    "Fourth: Summarize the trends in emotion recognition in this year at the end.\n",
    "<format>\n",
    "Session: session_name\n",
    "common themes: theme1, theme2, theme3\n",
    "Session: session_name\n",
    "common themes: theme1, theme2, theme3\n",
    "...\n",
    "</format>\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the chain\n",
    "# Now you can invoke the chain\n",
    "response = qa_chain.invoke(query)\n",
    "with open(\"interspeech2023.txt\", \"w\") as f:\n",
    "    f.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
